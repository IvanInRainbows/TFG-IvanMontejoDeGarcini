# Description

This is the repository used to store all the code used in the process of making my Bachelor's thesis (Trabajo de fin de grado) for the applied linguistics degree in the Complutense University of Madrid. The aim of the thesis is to help discerning texts that have been generated by language models from those which have been written by humans. This project focuses specifically on detecting reviews in Spanish generated by the language model gpt-4. A more specific objective of the thesis is to determine relevant linguistic features that characterise AI-generated and human-generated texts and to create a prototype that can be used for this purpose using Python and machine-learning libraries.

The subject of this thesis is motivated by the fact that AI-generated text and language have been a growing concern during the last years. This has been especially true in academic environments, where there has been several cases of assignments entirely created by AI. These are difficult to detect due to the lack accurate tools.

# Changelog

17/04/2025: Now the classifier has been finished (at least for now), new tests have been made regarding the accuracy of the feature order and algorithms. For that purpose two excel files have been created each containing the information on the accuracy of the algorithms and feature use. One of this files has been tested using the mutual info coefficient order for the features and the other one using point biserial. Finally added a description to the repo.

14/04/2025: Normalized most features (punctuation, wordCharCount, nSentsle11Words, nSentsge34Words, all syntactic features, Superlatives, Comparatives, all Bigram features, NERS and Grammar errors). Added TTR feature. Re-executed feature correlation to fit the changes made. Finished the model evaluation and subsequent classifier by optimizing the features to minimize model noise. As the train and test data are sampled randomly the best model may vary depending on the data. The sample taken has alfo been stratified so the quantity of each label is balanced between the test and train data. Exported the best model to a pickle file.

09/04/2025: Finished feature extraction by adding the text readability and legibility features, grammar errors and lexical diversity. Fixed some bugs in the nSentsle11Words and nSentsge34Words features. Added the feature correlation file where correlation between each feature and the label is calculated using point biserial and mutual information coefficients.

28/03/2025: Extended the dependency parser model evaluation to include more accurate metrics (precision, recall and f1 score). Given the computational processing power required the code has been adapted to Google Colab, as local runtime takes too long.

25/03/2025: Extended the dependency parser comparison between stanza and SpaCy there's information on the accuracy of stanza with the sentence tokenizer enabled. Extracted comparative and superlative structures for the Lexical features. Extracted the NER count for other features. Designed and extracted the syntactic features of order of constituents by checking the position of the subject and object relative to the verb or whether the subject is implicit. Added some comments for the sake of clarity and legibility.

08/03/2025: Completed the evaluation the accuracy of stanza and SpaCy when parsing syntactic structures. SpaCy's sentence tokenizer cant be disabled inside the Dependency Parser, so there might be some margin of error.

05/03/2025: Managed to make Freeling work and to take the output into a python object as a matrix of dictionaries (list[list[dict]]). The fist dimension constitutes the sentences, the second constitutes each word and each key of the dictionary refers to the following: ID FORM LEMMA TAG SHORT_TAG MSD NEC SENSE SYNTAX DEPHEAD DEPREL COREF SRL. Malt parser has been partially implemented but it depends on the freeling output.

03/03/2025: Started the evaluation of the syntactic parser that will be used for the Syntactic features. At least two parsers will be used (spacy and stanza). The third one might be MaltParser if I manage to make it work. Bash script to parse corpus with MaltParser written, but it doesn't get along with CoNLLu corpus annotation.

02/03/2025: Documented main Dataset class. Added POS bigram matrix DataFrame for each text as an attribute of the class, as well as two functions to initialize and process a bigram matrix given some keys. Added the following POS bigram features: adjective+verb, conjunction+verb, conjunction+noun, conjunction+adjective, conjunction+adverb, pronoun+verb, noun+verb. Also added capital letter count feature as well as some miscellaneous comments.

28/02/2025: Class created for the processing and storage of the dataset. Finished the extraction of the following word count features: number of sentences, number of words, average words per sentence, word character count, average word density, number of sentences with fewer than 12 words and number of sentences with more than 33 words. Extraction of the next lexical features: noun count, verb count, auxiliar verb count, adjective count, adverb count, conjugation count, preposition count and preposition count. Extraction of the punctuation count features.

22/02/2025: Initial commit.
