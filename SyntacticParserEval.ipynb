{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import stanza\n",
    "import spacy\n",
    "from stanza.utils.conll import CoNLL\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:35:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de270ed5b74840d6880aace64114cb7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-05 17:35:14 INFO: Downloaded file to /home/ivan/stanza_resources/resources.json\n",
      "2025-03-05 17:35:15 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | conll02           |\n",
      "=================================\n",
      "\n",
      "2025-03-05 17:35:15 INFO: Using device: cpu\n",
      "2025-03-05 17:35:15 INFO: Loading: tokenize\n",
      "2025-03-05 17:35:17 INFO: Loading: mwt\n",
      "2025-03-05 17:35:17 INFO: Loading: pos\n",
      "2025-03-05 17:35:19 INFO: Loading: lemma\n",
      "2025-03-05 17:35:21 INFO: Loading: depparse\n",
      "2025-03-05 17:35:21 INFO: Loading: ner\n",
      "2025-03-05 17:35:24 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp_stanza = stanza.Pipeline(lang='es', processors='tokenize,ner,mwt,pos,lemma,depparse')\n",
    "nlp_spacy = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineFilePath = \"/home/ivan/Desktop/Uni/TFG/RepositorioTFG/Datasets/UD_Spanish-AnCora/es_ancora-ud-test.conllu\"\n",
    "baseline = CoNLL.conll2doc(baselineFilePath)\n",
    "sentences = [i.text for i in baseline.sentences[:]]\n",
    "\n",
    "# SPACY MODEL\n",
    "spacy_output = nlp_spacy(\" \".join(sentences))\n",
    "\n",
    "# STANZA MODEL\n",
    "stanza_output = nlp_stanza(\" \".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output saved in /home/ivan/Desktop/Uni/TFG/RepositorioTFG/freelingOutput.conll\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Execute this only if you have installed Freeling!!\n",
    "# FREELING MODEL\n",
    "freeling_output_path = os.path.abspath(\"\")+\"/freelingOutput.conll\" #Path for the output of freeling, change this if you want your output to be saved somewhere else\n",
    "_tmp = open(os.path.abspath(\"\")+\"/tmp.txt\", \"w+\").write(\" \".join(sentences))\n",
    "os.system(f\"bash ./callFreeling.sh {os.path.abspath(\"\")+\"/tmp.txt\"} {freeling_output_path}\")\n",
    "os.remove(\"tmp.txt\")\n",
    "freeling_parser = open(freeling_output_path, \"r\").read().split(\"\\n\\n\")\n",
    "freeling_parser = [i.split(\"\\n\") for i in freeling_parser]\n",
    "columns = \"ID FORM LEMMA TAG SHORT_TAG MSD NEC SENSE SYNTAX DEPHEAD DEPREL COREF SRL\".split(\" \")\n",
    "freeling_parser = [[[{columns[i] : re.split(\" +\", word)[i]} for i in range(len(re.split(\" +\", word)))] for word in sent]for sent in freeling_parser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ivan/Desktop/Uni/TFG/RepositorioTFG/corpus.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "                          MaltParser 1.9.2                             \n",
      "-----------------------------------------------------------------------------\n",
      "         MALT (Models and Algorithms for Language Technology) Group          \n",
      "             Vaxjo University and Uppsala University                         \n",
      "                             Sweden                                          \n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "Started: Wed Mar 05 17:46:57 CET 2025\n",
      "  Transition system    : Non-Projective\n",
      "  Parser configuration : Covington with allow_root=true and allow_shift=false\n",
      "  Data Format          : /espmalt-1.0/conllx.xml\n",
      ".          \t      1\t      2s\t    145MB\n",
      ".          \t     10\t      2s\t    145MB\n",
      ".          \t    100\t      3s\t    190MB\n",
      "..........\t   1000\t     12s\t    365MB\n",
      ".......    \t   1721\t     15s\t    304MB\n",
      "Parsing time: 00:00:14 (14550 ms)\n",
      "Finished: Wed Mar 05 17:47:12 CET 2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output saved in /home/ivan/Desktop/Uni/TFG/RepositorioTFG/maltParserOut.conll\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Execute this only if you have installed Malt Parser!!\n",
    "# NOTE: Also check that the Malt Parser folder is in the same directory as the callMaltParser.sh script\n",
    "# MALT PARSER: chances are this will be removed, as I can't get it to work properly and it depends on freeling sentencizer and tagger\n",
    "if baselineFilePath[len(baselineFilePath)-6:] != \".conll\":\n",
    "    os.system(f\"python conllu-to-conll-standard.py {baselineFilePath} {os.path.abspath(\"\")+\"/corpus.conll\"}\")\n",
    "    os.system(f\"bash callMaltParser.sh {os.path.abspath(\"\")+\"/corpus.conll\"}\")\n",
    "else:\n",
    "    os.system(f\"bash callMaltParser.sh {baselineFilePath}\")\n",
    "malt_parser = CoNLL.conll2doc(os.path.abspath(\"\")+\"/maltParserOut.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:  1721 sentences\n",
      "Spacy: 1714 sentences. Baseline difference:  -0.40674026728646506 %\n",
      "Stanza: 1724 sentences. Baseline difference:  0.17431725740848503 %\n",
      "freeling: 1495 sentences. Baseline difference:  -13.13190005810575 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline: \", len(list(baseline.sentences)), \"sentences\")\n",
    "#print(\"Malt Parser: \", len(list(malt_parser.sentences)), \"sentences\")\n",
    "print(f\"Spacy: {len(list(spacy_output.sents))} sentences. Baseline difference: \", (len(list(spacy_output.sents))/len(list(baseline.sentences))-1)*100, \"%\")\n",
    "print(f\"Stanza: {len(list(stanza_output.sentences))} sentences. Baseline difference: \", (len(list(stanza_output.sentences))/len(list(baseline.sentences))-1)*100, \"%\")\n",
    "print(f\"freeling: {len(freeling_parser)} sentences. Baseline difference: \", (len(freeling_parser)/len(list(baseline.sentences))-1)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence split is inconsistent between stanza, spacy and especially freeling when compared to the baseline, which is a real problem. This highlights the difficulty of comparing syntactic parsers, as there are many more systems that play a role such as PoS tagger and sentencizer. A different approach is required, maybe disabling the sentencizer and processing the corpus sentence by sentence?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
