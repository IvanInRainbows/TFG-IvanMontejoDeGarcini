{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "import os\n",
    "import re\n",
    "\n",
    "def callFreeling(text :str, splitSentences = True) -> list[list[dict]]:\n",
    "    open(os.path.abspath(\"\")+\"/tmp.txt\", \"w+\").write(text)\n",
    "    os.system(f\"bash ./callFreeling.sh {os.path.abspath(\"\")+\"/tmp.txt\"} {os.path.abspath(\"\")+\"/tmp2.txt\"}\")\n",
    "    out = open(os.path.abspath(\"\")+\"/tmp2.txt\", \"r\").read().split(\"\\n\\n\") if splitSentences else [open(os.path.abspath(\"\")+\"/tmp2.txt\", \"r\").read().replace(\"\\n\\n\", \"\\n\")]\n",
    "    os.remove(os.path.abspath(\"\")+\"/tmp.txt\")\n",
    "    os.remove(os.path.abspath(\"\")+\"/tmp2.txt\")\n",
    "    out = [i.split(\"\\n\") for i in out]\n",
    "    columns = \"ID FORM LEMMA TAG SHORT_TAG MSD NEC SENSE SYNTAX DEPHEAD DEPREL COREF SRL\".split(\" \")\n",
    "    out = [[{columns[i] : re.split(\" +\", word)[i] for i in range(len(re.split(\" +\", word)))} for word in sent]for sent in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 12:37:36 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d133eae087414c5b93dcb874f42ddd07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 12:37:36 INFO: Downloaded file to /home/ivan/stanza_resources/resources.json\n",
      "2025-03-08 12:37:38 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | conll02           |\n",
      "=================================\n",
      "\n",
      "2025-03-08 12:37:38 INFO: Using device: cpu\n",
      "2025-03-08 12:37:38 INFO: Loading: tokenize\n",
      "2025-03-08 12:37:40 INFO: Loading: mwt\n",
      "2025-03-08 12:37:40 INFO: Loading: pos\n",
      "2025-03-08 12:37:43 INFO: Loading: lemma\n",
      "2025-03-08 12:37:45 INFO: Loading: depparse\n",
      "2025-03-08 12:37:45 INFO: Loading: ner\n",
      "2025-03-08 12:37:47 INFO: Done loading processors!\n",
      "/home/ivan/miniconda3/lib/python3.12/site-packages/thinc/shims/pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "nlp_stanza = stanza.Pipeline(lang='es', processors='tokenize,ner,mwt,pos,lemma,depparse')\n",
    "nlp_spacy = spacy.load(\"es_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineFilePath = \"/home/ivan/Desktop/Uni/TFG/RepositorioTFG/Datasets/UD_Spanish-AnCora/es_ancora-ud-test.conllu\" #Baseline corpus path. It's not included in the repo, change this at your own will. Must be in conllu format\n",
    "baseline = CoNLL.conll2doc(baselineFilePath)\n",
    "sentences = [i.text for i in baseline.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY MODEL\n",
    "spacy_output = nlp_spacy(\" \".join(sentences))\n",
    "\n",
    "# STANZA MODEL\n",
    "stanza_output = nlp_stanza(\" \".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output saved in /home/ivan/Desktop/Uni/TFG/RepositorioTFG/freelingOutput.conll\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Execute this only if you have installed Freeling!!\n",
    "# FREELING MODEL\n",
    "freeling_parser = callFreeling(\" \".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ivan/Desktop/Uni/TFG/RepositorioTFG/corpus.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "                          MaltParser 1.9.2                             \n",
      "-----------------------------------------------------------------------------\n",
      "         MALT (Models and Algorithms for Language Technology) Group          \n",
      "             Vaxjo University and Uppsala University                         \n",
      "                             Sweden                                          \n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "Started: Wed Mar 05 17:46:57 CET 2025\n",
      "  Transition system    : Non-Projective\n",
      "  Parser configuration : Covington with allow_root=true and allow_shift=false\n",
      "  Data Format          : /espmalt-1.0/conllx.xml\n",
      ".          \t      1\t      2s\t    145MB\n",
      ".          \t     10\t      2s\t    145MB\n",
      ".          \t    100\t      3s\t    190MB\n",
      "..........\t   1000\t     12s\t    365MB\n",
      ".......    \t   1721\t     15s\t    304MB\n",
      "Parsing time: 00:00:14 (14550 ms)\n",
      "Finished: Wed Mar 05 17:47:12 CET 2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output saved in /home/ivan/Desktop/Uni/TFG/RepositorioTFG/maltParserOut.conll\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Execute this only if you have installed Malt Parser!!\n",
    "# NOTE: Also check that the Malt Parser folder is in the same directory as the callMaltParser.sh script\n",
    "# MALT PARSER: chances are this will be removed, as I can't get it to work properly and it depends on freeling sentencizer and tagger\n",
    "if baselineFilePath[len(baselineFilePath)-6:] != \".conll\":\n",
    "    os.system(f\"python conllu-to-conll-standard.py {baselineFilePath} {os.path.abspath(\"\")+\"/corpus.conll\"}\")\n",
    "    os.system(f\"bash callMaltParser.sh {os.path.abspath(\"\")+\"/corpus.conll\"}\")\n",
    "else:\n",
    "    os.system(f\"bash callMaltParser.sh {baselineFilePath}\")\n",
    "malt_parser = CoNLL.conll2doc(os.path.abspath(\"\")+\"/maltParserOut.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:  1721 sentences\n",
      "Spacy: 1714 sentences. Baseline difference:  -0.40674026728646506 %\n",
      "Stanza: 1724 sentences. Baseline difference:  0.17431725740848503 %\n",
      "freeling: 1495 sentences. Baseline difference:  -13.13190005810575 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline: \", len(list(baseline.sentences)), \"sentences\")\n",
    "#print(\"Malt Parser: \", len(list(malt_parser.sentences)), \"sentences\")\n",
    "print(f\"Spacy: {len(list(spacy_output.sents))} sentences. Baseline difference: \", (len(list(spacy_output.sents))/len(list(baseline.sentences))-1)*100, \"%\")\n",
    "print(f\"Stanza: {len(list(stanza_output.sentences))} sentences. Baseline difference: \", (len(list(stanza_output.sentences))/len(list(baseline.sentences))-1)*100, \"%\")\n",
    "print(f\"freeling: {len(freeling_parser)} sentences. Baseline difference: \", (len(freeling_parser)/len(list(baseline.sentences))-1)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence split is inconsistent between stanza, spacy and especially freeling when compared to the baseline, which is a real problem. This highlights the difficulty of comparing syntactic parsers, as there are many more systems that play a role such as PoS tagger and sentencizer. A different approach is required, maybe disabling the sentencizer and processing the corpus sentence by sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 12:38:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4298b54c085e4d3a82687681a4ff901a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 12:38:01 INFO: Downloaded file to /home/ivan/stanza_resources/resources.json\n",
      "2025-03-08 12:38:03 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | conll02           |\n",
      "=================================\n",
      "\n",
      "2025-03-08 12:38:03 INFO: Using device: cpu\n",
      "2025-03-08 12:38:03 INFO: Loading: tokenize\n",
      "2025-03-08 12:38:03 INFO: Loading: mwt\n",
      "2025-03-08 12:38:03 INFO: Loading: pos\n",
      "2025-03-08 12:38:05 INFO: Loading: lemma\n",
      "2025-03-08 12:38:06 INFO: Loading: depparse\n",
      "2025-03-08 12:38:07 INFO: Loading: ner\n",
      "2025-03-08 12:38:09 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#Trying only dependence parsing\n",
    "\n",
    "# Loading stanza model with sentence split disabled\n",
    "nlp_stanza = stanza.Pipeline(lang='es', processors='tokenize,ner,mwt,pos,lemma,depparse', tokenize_no_ssplit=True)\n",
    "\n",
    "# Spacy sentencizer apparently can't be disabled so each sentence in the corpus is processed individually instead. Note that this may risk sentence segmenter interference\n",
    "nlp_spacy = spacy.load(\"es_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_output = nlp_stanza(\"\\n\\n\".join(sentences))\n",
    "tag_dict = {} # Index 0 for count; index 1 for correct tag in model\n",
    "for i in range(len(list(stanza_output.sentences))):\n",
    "    if baseline.sentences[i].text != stanza_output.sentences[i].text:\n",
    "        raise KeyError(\"Sentences are not equal\")\n",
    "    for j in range(len(list(baseline.sentences[i].words))):\n",
    "        try:\n",
    "            if baseline.sentences[i].words[j].deprel not in tag_dict.keys():\n",
    "                tag_dict.update({baseline.sentences[i].words[j].deprel : [1,0]})\n",
    "            else: tag_dict[baseline.sentences[i].words[j].deprel][0] += 1 \n",
    "            if baseline.sentences[i].words[j].deprel == stanza_output.sentences[i].words[j].deprel:\n",
    "                tag_dict[baseline.sentences[i].words[j].deprel][1] += 1 \n",
    "        except IndexError:\n",
    "            if baseline.sentences[i].words[j].deprel not in tag_dict.keys():\n",
    "                tag_dict.update({baseline.sentences[i].words[j].deprel : [1,0]})\n",
    "            else: tag_dict[baseline.sentences[i].words[j].deprel][0] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stanza in Spanish subjects, objects and roots:\n",
      "\tnominal subjects: 0.9361924686192469\n",
      "\tclause subjects: 0.6595744680851063\n",
      "\tobjects: 0.8522920203735145\n",
      "\troot of sentence: 0.9517722254503196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = {k:tag_dict[k][1]/tag_dict[k][0] for k in tag_dict.keys()}\n",
    "print(f\"\"\"Accuracy of Stanza in Spanish subjects, objects and roots:\n",
    "\\tnominal subjects: {accuracy[\"nsubj\"]}\n",
    "\\tclause subjects: {accuracy[\"csubj\"]}\n",
    "\\tobjects: {accuracy[\"obj\"]}\n",
    "\\troot of sentence: {accuracy[\"root\"]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/miniconda3/lib/python3.12/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "tag_dict = {}\n",
    "neqSents = 0\n",
    "for i in range(len(baseline.sentences)):\n",
    "    spacy_output = nlp_spacy(baseline.sentences[i].text.replace(\" del \", \" de el \"))\n",
    "    if baseline.sentences[i].text != list(spacy_output.sents)[0].text:\n",
    "         neqSents+=1\n",
    "    for j in range(len(baseline.sentences[i].words)):\n",
    "        if baseline.sentences[i].words[j].deprel not in tag_dict.keys():\n",
    "                tag_dict.update({baseline.sentences[i].words[j].deprel : [1,0]})\n",
    "        else: tag_dict[baseline.sentences[i].words[j].deprel][0] += 1 \n",
    "        if j < len(list(spacy_output.sents)[0]):\n",
    "            \"\"\"print(f\"{baseline.sentences[i].words[j].text} / {list(spacy_output.sents)[0][j+offset].text} with deprel {baseline.sentences[i].words[j].deprel} / {list(spacy_output.sents)[0][j+offset].dep_.lower()}\")\"\"\" # This is for debugging purposes\n",
    "            if baseline.sentences[i].words[j].deprel == list(spacy_output.sents)[0][j].dep_.lower():\n",
    "                tag_dict[baseline.sentences[i].words[j].deprel][1] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Spacy in Spanish subjects, objects and roots:\n",
      "\tSubjects: 0.8657601115760112\n",
      "\tObjects: 0.7461799660441426\n",
      "\tRoot of sentence: 0.9151656013945381\n",
      "\n",
      "484 sentences were not equal, maybe due to a segmentation problem.\n"
     ]
    }
   ],
   "source": [
    "accuracy = {k:tag_dict[k][1]/tag_dict[k][0] for k in tag_dict.keys()}\n",
    "print(f\"\"\"Accuracy of Spacy in Spanish subjects, objects and roots:\n",
    "\\tSubjects: {accuracy[\"nsubj\"]}\n",
    "\\tObjects: {accuracy[\"obj\"]}\n",
    "\\tRoot of sentence: {accuracy[\"root\"]}\n",
    "\"\"\")\n",
    "\n",
    "print(f\"{neqSents} sentences were not equal, maybe due to a segmentation problem.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
