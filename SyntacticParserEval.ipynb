{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import stanza\n",
    "from stanza.utils.conll import CoNLL\n",
    "import os\n",
    "import re\n",
    "\n",
    "def callFreeling(text :str, splitSentences = True) -> list[list[dict]]:\n",
    "    open(os.path.abspath(\"\")+\"/tmp.txt\", \"w+\").write(text)\n",
    "    os.system(f\"bash ./callFreeling.sh {os.path.abspath(\"\")+\"/tmp.txt\"} {os.path.abspath(\"\")+\"/tmp2.txt\"}\")\n",
    "    out = open(os.path.abspath(\"\")+\"/tmp2.txt\", \"r\").read().split(\"\\n\\n\") if splitSentences else [open(os.path.abspath(\"\")+\"/tmp2.txt\", \"r\").read().replace(\"\\n\\n\", \"\\n\")]\n",
    "    os.remove(os.path.abspath(\"\")+\"/tmp.txt\")\n",
    "    os.remove(os.path.abspath(\"\")+\"/tmp2.txt\")\n",
    "    out = [i.split(\"\\n\") for i in out]\n",
    "    columns = \"ID FORM LEMMA TAG SHORT_TAG MSD NEC SENSE SYNTAX DEPHEAD DEPREL COREF SRL\".split(\" \")\n",
    "    out = [[{columns[i] : re.split(\" +\", word)[i] for i in range(len(re.split(\" +\", word)))} for word in sent]for sent in out]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 20:49:51 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4319a72a2a984f728a530279f4568190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-23 20:49:52 INFO: Downloaded file to /home/ivan/stanza_resources/resources.json\n",
      "2025-03-23 20:49:53 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | conll02           |\n",
      "=================================\n",
      "\n",
      "2025-03-23 20:49:53 INFO: Using device: cpu\n",
      "2025-03-23 20:49:53 INFO: Loading: tokenize\n",
      "2025-03-23 20:49:54 INFO: Loading: mwt\n",
      "2025-03-23 20:49:54 INFO: Loading: pos\n",
      "2025-03-23 20:49:56 INFO: Loading: lemma\n",
      "2025-03-23 20:49:57 INFO: Loading: depparse\n",
      "2025-03-23 20:49:58 INFO: Loading: ner\n",
      "2025-03-23 20:50:00 INFO: Done loading processors!\n",
      "/home/ivan/miniconda3/lib/python3.12/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'es_dep_news_trf' (3.7.2) was trained with spaCy v3.7.0 and may not be 100% compatible with the current version (3.8.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n",
      "/home/ivan/miniconda3/lib/python3.12/site-packages/thinc/shims/pytorch.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "nlp_stanza = stanza.Pipeline(lang='es', processors='tokenize,ner,mwt,pos,lemma,depparse')\n",
    "nlp_spacy = spacy.load(\"es_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "baselineFilePath = \"/home/ivan/Desktop/Uni/TFG/RepositorioTFG/Datasets/UD_Spanish-AnCora/es_ancora-ud-train.conllu\" #Baseline corpus path. It's not included in the repo, change this at your own will. Must be in conllu format\n",
    "baseline = CoNLL.conll2doc(baselineFilePath)\n",
    "sentences = [i.text for i in baseline.sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPACY MODEL\n",
    "spacy_output = nlp_spacy(\" \".join(sentences))\n",
    "\n",
    "# STANZA MODEL\n",
    "stanza_output = nlp_stanza(\" \".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output saved in /home/ivan/Desktop/Uni/TFG/RepositorioTFG/freelingOutput.conll\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Execute this only if you have installed Freeling!!\n",
    "# FREELING MODEL\n",
    "freeling_parser = callFreeling(\" \".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ivan/Desktop/Uni/TFG/RepositorioTFG/corpus.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "                          MaltParser 1.9.2                             \n",
      "-----------------------------------------------------------------------------\n",
      "         MALT (Models and Algorithms for Language Technology) Group          \n",
      "             Vaxjo University and Uppsala University                         \n",
      "                             Sweden                                          \n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      "Started: Wed Mar 05 17:46:57 CET 2025\n",
      "  Transition system    : Non-Projective\n",
      "  Parser configuration : Covington with allow_root=true and allow_shift=false\n",
      "  Data Format          : /espmalt-1.0/conllx.xml\n",
      ".          \t      1\t      2s\t    145MB\n",
      ".          \t     10\t      2s\t    145MB\n",
      ".          \t    100\t      3s\t    190MB\n",
      "..........\t   1000\t     12s\t    365MB\n",
      ".......    \t   1721\t     15s\t    304MB\n",
      "Parsing time: 00:00:14 (14550 ms)\n",
      "Finished: Wed Mar 05 17:47:12 CET 2025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output saved in /home/ivan/Desktop/Uni/TFG/RepositorioTFG/maltParserOut.conll\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Execute this only if you have installed Malt Parser!!\n",
    "# NOTE: Also check that the Malt Parser folder is in the same directory as the callMaltParser.sh script\n",
    "# MALT PARSER: chances are this will be removed, as I can't get it to work properly and it depends on freeling sentencizer and tagger\n",
    "if baselineFilePath[len(baselineFilePath)-6:] != \".conll\":\n",
    "    os.system(f\"python conllu-to-conll-standard.py {baselineFilePath} {os.path.abspath(\"\")+\"/corpus.conll\"}\")\n",
    "    os.system(f\"bash callMaltParser.sh {os.path.abspath(\"\")+\"/corpus.conll\"}\")\n",
    "else:\n",
    "    os.system(f\"bash callMaltParser.sh {baselineFilePath}\")\n",
    "malt_parser = CoNLL.conll2doc(os.path.abspath(\"\")+\"/maltParserOut.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline:  1721 sentences\n",
      "Spacy: 1714 sentences. Baseline difference:  -0.40674026728646506 %\n",
      "Stanza: 1724 sentences. Baseline difference:  0.17431725740848503 %\n",
      "freeling: 1495 sentences. Baseline difference:  -13.13190005810575 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline: \", len(list(baseline.sentences)), \"sentences\")\n",
    "#print(\"Malt Parser: \", len(list(malt_parser.sentences)), \"sentences\")\n",
    "print(f\"Spacy: {len(list(spacy_output.sents))} sentences. Baseline difference: \", (len(list(spacy_output.sents))/len(list(baseline.sentences))-1)*100, \"%\")\n",
    "print(f\"Stanza: {len(list(stanza_output.sentences))} sentences. Baseline difference: \", (len(list(stanza_output.sentences))/len(list(baseline.sentences))-1)*100, \"%\")\n",
    "print(f\"freeling: {len(freeling_parser)} sentences. Baseline difference: \", (len(freeling_parser)/len(list(baseline.sentences))-1)*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence split is inconsistent between stanza, spacy and especially freeling when compared to the baseline, which is a real problem. This highlights the difficulty of comparing syntactic parsers, as there are many more systems that play a role such as PoS tagger and sentencizer. A different approach is required, maybe disabling the sentencizer and processing the corpus sentence by sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 12:38:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4298b54c085e4d3a82687681a4ff901a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 12:38:01 INFO: Downloaded file to /home/ivan/stanza_resources/resources.json\n",
      "2025-03-08 12:38:03 INFO: Loading these models for language: es (Spanish):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "| depparse  | combined_charlm   |\n",
      "| ner       | conll02           |\n",
      "=================================\n",
      "\n",
      "2025-03-08 12:38:03 INFO: Using device: cpu\n",
      "2025-03-08 12:38:03 INFO: Loading: tokenize\n",
      "2025-03-08 12:38:03 INFO: Loading: mwt\n",
      "2025-03-08 12:38:03 INFO: Loading: pos\n",
      "2025-03-08 12:38:05 INFO: Loading: lemma\n",
      "2025-03-08 12:38:06 INFO: Loading: depparse\n",
      "2025-03-08 12:38:07 INFO: Loading: ner\n",
      "2025-03-08 12:38:09 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "#Trying only dependence parsing\n",
    "\n",
    "# Loading stanza model with sentence split disabled\n",
    "nlp_stanza = stanza.Pipeline(lang='es', processors='tokenize,ner,mwt,pos,lemma,depparse', tokenize_no_ssplit=True)\n",
    "\n",
    "# Spacy sentencizer apparently can't be disabled so each sentence in the corpus is processed individually instead. Note that this may risk sentence segmenter interference\n",
    "nlp_spacy = spacy.load(\"es_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m stanza_output \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_stanza\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m tag_dict \u001b[38;5;241m=\u001b[39m {} \u001b[38;5;66;03m# Dictionary with a list of length 2 inside. Index 0 for count; index 1 for correct tag in model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(stanza_output\u001b[38;5;241m.\u001b[39msentences))):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/stanza/pipeline/core.py:480\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, processors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/stanza/pipeline/core.py:431\u001b[0m, in \u001b[0;36mPipeline.process\u001b[0;34m(self, doc, processors)\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mget(processor_name):\n\u001b[1;32m    430\u001b[0m         process \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mbulk_process \u001b[38;5;28;01mif\u001b[39;00m bulk \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors[processor_name]\u001b[38;5;241m.\u001b[39mprocess\n\u001b[0;32m--> 431\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m doc\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/stanza/pipeline/depparse_processor.py:65\u001b[0m, in \u001b[0;36mDepparseProcessor.process\u001b[0;34m(self, document)\u001b[0m\n\u001b[1;32m     63\u001b[0m     preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch):\n\u001b[0;32m---> 65\u001b[0m         preds \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mdata_orig_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     preds \u001b[38;5;241m=\u001b[39m unsort(preds, batch\u001b[38;5;241m.\u001b[39mdata_orig_idx)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/stanza/models/depparse/trainer.py:149\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, batch, unsort)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    148\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m word\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordchars_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mufeats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeprel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_orig_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwordlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m head_seqs \u001b[38;5;241m=\u001b[39m [chuliu_edmonds_one_root(adj[:l, :l])[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;28;01mfor\u001b[39;00m adj, l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(preds[\u001b[38;5;241m0\u001b[39m], sentlens)] \u001b[38;5;66;03m# remove attachment for the root\u001b[39;00m\n\u001b[1;32m    151\u001b[0m deprel_seqs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeprel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munmap([preds[\u001b[38;5;241m1\u001b[39m][i][j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m][h] \u001b[38;5;28;01mfor\u001b[39;00m j, h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(hs)]) \u001b[38;5;28;01mfor\u001b[39;00m i, hs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(head_seqs)]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/stanza/models/depparse/model.py:174\u001b[0m, in \u001b[0;36mParser.forward\u001b[0;34m(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, lemma, head, deprel, word_orig_idx, sentlens, wordlens, text)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharlm\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# \\n is to add a somewhat neutral \"word\" for the ROOT\u001b[39;00m\n\u001b[1;32m    173\u001b[0m     charlm_text \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m text]\n\u001b[0;32m--> 174\u001b[0m     all_forward_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmodel_forward\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_char_representation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcharlm_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     all_forward_chars \u001b[38;5;241m=\u001b[39m pack(pad_sequence(all_forward_chars, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m    176\u001b[0m     all_backward_chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharmodel_backward\u001b[38;5;241m.\u001b[39mbuild_char_representation(charlm_text)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/stanza/models/common/char_model.py:222\u001b[0m, in \u001b[0;36mCharacterLanguageModel.build_char_representation\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    219\u001b[0m chars \u001b[38;5;241m=\u001b[39m get_long_tensor(chars, \u001b[38;5;28mlen\u001b[39m(all_data), pad_id\u001b[38;5;241m=\u001b[39mvocab\u001b[38;5;241m.\u001b[39munit2id(CHARLM_END))\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 222\u001b[0m     output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchars\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar_lens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     res \u001b[38;5;241m=\u001b[39m [output[i, offsets] \u001b[38;5;28;01mfor\u001b[39;00m i, offsets \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(char_offsets)]\n\u001b[1;32m    224\u001b[0m     res \u001b[38;5;241m=\u001b[39m unsort(res, orig_idx)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/stanza/models/common/char_model.py:153\u001b[0m, in \u001b[0;36mCharacterLanguageModel.forward\u001b[0;34m(self, chars, charlens, hidden)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \n\u001b[1;32m    151\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_h_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous(),\n\u001b[1;32m    152\u001b[0m               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcharlstm_c_init\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_num_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchar_hidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[0;32m--> 153\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcharlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pad_packed_sequence(output, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    155\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(output)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/stanza/models/common/packed_lstm.py:22\u001b[0m, in \u001b[0;36mPackedLSTM.forward\u001b[0;34m(self, input, lengths, hx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, PackedSequence):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m pack_padded_sequence(\u001b[38;5;28minput\u001b[39m, lengths, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n\u001b[0;32m---> 22\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad:\n\u001b[1;32m     24\u001b[0m     res \u001b[38;5;241m=\u001b[39m (pad_packed_sequence(res[\u001b[38;5;241m0\u001b[39m], batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)[\u001b[38;5;241m0\u001b[39m], res[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1135\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\n\u001b[1;32m   1124\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1125\u001b[0m         hx,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1133\u001b[0m     )\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1135\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1140\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1146\u001b[0m output \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1147\u001b[0m hidden \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "stanza_output = nlp_stanza(\"\\n\\n\".join(sentences))\n",
    "tag_dict = {} # Dictionary with a list of length 2 inside. Index 0 for count; index 1 for correct tag in model\n",
    "for i in range(len(list(stanza_output.sentences))):\n",
    "    if baseline.sentences[i].text != stanza_output.sentences[i].text:\n",
    "        raise KeyError(\"Sentences are not equal\")\n",
    "    for j in range(len(list(baseline.sentences[i].words))):\n",
    "        try:\n",
    "            if baseline.sentences[i].words[j].deprel not in tag_dict.keys():\n",
    "                tag_dict.update({baseline.sentences[i].words[j].deprel : [1,0]}) \n",
    "            else: tag_dict[baseline.sentences[i].words[j].deprel][0] += 1 \n",
    "            \n",
    "            if baseline.sentences[i].words[j].deprel == stanza_output.sentences[i].words[j].deprel:\n",
    "                tag_dict[baseline.sentences[i].words[j].deprel][1] += 1 \n",
    "        except IndexError:\n",
    "            if baseline.sentences[i].words[j].deprel not in tag_dict.keys():\n",
    "                tag_dict.update({baseline.sentences[i].words[j].deprel : [1,0]})\n",
    "            else: tag_dict[baseline.sentences[i].words[j].deprel][0] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Stanza in Spanish subjects, objects and roots:\n",
      "\tnominal subjects: 0.9361924686192469\n",
      "\tclause subjects: 0.6595744680851063\n",
      "\tobjects: 0.8522920203735145\n",
      "\troot of sentence: 0.9517722254503196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = {k:tag_dict[k][1]/tag_dict[k][0] for k in tag_dict.keys()}\n",
    "print(f\"\"\"Accuracy of Stanza in Spanish subjects, objects and roots:\n",
    "\\tnominal subjects: {accuracy[\"nsubj\"]}\n",
    "\\tclause subjects: {accuracy[\"csubj\"]}\n",
    "\\tobjects: {accuracy[\"obj\"]}\n",
    "\\troot of sentence: {accuracy[\"root\"]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ivan/miniconda3/lib/python3.12/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "tag_dict = {}\n",
    "neqSents = 0\n",
    "for i in range(len(baseline.sentences)):\n",
    "    spacy_output = nlp_spacy(baseline.sentences[i].text.replace(\" del \", \" de el \"))\n",
    "    if baseline.sentences[i].text != list(spacy_output.sents)[0].text:\n",
    "         neqSents+=1\n",
    "    for j in range(len(baseline.sentences[i].words)):\n",
    "        if baseline.sentences[i].words[j].deprel not in tag_dict.keys():\n",
    "                tag_dict.update({baseline.sentences[i].words[j].deprel : [1,0]})\n",
    "        else: tag_dict[baseline.sentences[i].words[j].deprel][0] += 1 \n",
    "        if j < len(list(spacy_output.sents)[0]):\n",
    "            \"\"\"print(f\"{baseline.sentences[i].words[j].text} / {list(spacy_output.sents)[0][j+offset].text} with deprel {baseline.sentences[i].words[j].deprel} / {list(spacy_output.sents)[0][j+offset].dep_.lower()}\")\"\"\" # This is for debugging purposes\n",
    "            if baseline.sentences[i].words[j].deprel == list(spacy_output.sents)[0][j].dep_.lower():\n",
    "                tag_dict[baseline.sentences[i].words[j].deprel][1] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Spacy in Spanish subjects, objects and roots:\n",
      "\tSubjects: 0.8657601115760112\n",
      "\tObjects: 0.7461799660441426\n",
      "\tRoot of sentence: 0.9151656013945381\n",
      "\n",
      "484 sentences were not equal, maybe due to a segmentation problem.\n"
     ]
    }
   ],
   "source": [
    "accuracy = {k:tag_dict[k][1]/tag_dict[k][0] for k in tag_dict.keys()}\n",
    "print(f\"\"\"Accuracy of Spacy in Spanish subjects, objects and roots:\n",
    "\\tSubjects: {accuracy[\"nsubj\"]}\n",
    "\\tObjects: {accuracy[\"obj\"]}\n",
    "\\tRoot of sentence: {accuracy[\"root\"]}\n",
    "\"\"\")\n",
    "\n",
    "print(f\"{neqSents} sentences were not equal, maybe due to a segmentation problem.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
